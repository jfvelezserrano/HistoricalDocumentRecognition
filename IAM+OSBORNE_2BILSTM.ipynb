{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antes de empezar\n",
    "\n",
    "conda activate python3.6_cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "\n",
    "from scipy import ndimage\n",
    "import math\n",
    "import random\n",
    "import skimage \n",
    "import h5py\n",
    "\n",
    "import cProfile, pstats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Ignore harmless warnings:\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1\n",
      "3.6.10\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(torch.__version__)\n",
    "print(platform.python_version())\n",
    "torch.cuda.get_device_name(1)\n",
    "path_IAM = \"/home/abarreiro/data/handwriting/seq2seq/IAM_words_48_192.hdf5\"\n",
    "\n",
    "path_OSB = \"/home/abarreiro/data/handwriting/seq2seq/OSBORNE_words_48_192.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definiendo diccionario, codificación y longitud máxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "# Dictionary used in seq2seq paper\n",
    "decoder_dict = {0: '0', 1: '!', 2: 'L', 3: 'z', 4: 'G', 5: 'm', 6: '6', 7: '/', 8: 'j', 9: 's', 10: 'S', 11: '5',\n",
    "                12: 'R', 13: ')', 14: 'u', 15: 'y', 16: '9', 17: 'g', 18: '3', 19: '1', 20: 'e', 21: \"'\", 22: ':',\n",
    "                23: 'Q', 24: '2', 25: 'a', 26: 't', 27: 'A', 28: '7', 29: ';', 30: 'i', 31: 'H', 32: 'W', 33: ',',\n",
    "                34: '(', 35: 'O', 36: 'U', 37: 'K', 38: 'd', 39: '*', 40: '.', 41: '?', 42: 'q', 43: '-', 44: 'r',\n",
    "                45: 'n', 46: '&', 47: 'C', 48: '\"', 49: 'h', 50: 'v', 51: 'f', 52: 'E', 53: 'p', 54: 'x', 55: '+',\n",
    "                56: 'w', 57: 'b', 58: 'o', 59: ' ', 60: 'B', 61: 'P', 62: 'D', 63: 'I', 64: 'J', 65: 'V', 66: 'N',\n",
    "                67: 'M', 68: '8', 69: 'k', 70: 'c', 71: '4', 72: 'T', 73: 'X', 74: 'l', 75: 'Z', 76: 'F', 77: 'Y',\n",
    "                78: 'START', 79: 'END', 80: 'PAD'}\n",
    "\n",
    "inverse_decoder_dict = {v: k for k, v in decoder_dict.items()}\n",
    "print(inverse_decoder_dict['END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]])\n",
      "80\n",
      "PAD\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# One_hot_mapping assigns to each number in decoder_dict its corresponding one-hot vector:\n",
    "\n",
    "one_hot_mapping = {}\n",
    "\n",
    "cont = 0\n",
    "for item in decoder_dict:\n",
    "    vector = torch.zeros(1, 1, len(decoder_dict))\n",
    "    vector[0, 0, cont] = 1.0\n",
    "    one_hot_mapping[item] = vector\n",
    "    cont += 1\n",
    "\n",
    "# Inverse_one_hot_mapping assigns to each one-hot vector its corresponding number in decoder_dict\n",
    "inverse_one_hot_mapping = {v: k for k, v in one_hot_mapping.items()}\n",
    "\n",
    "# One_hot_to_char assigns to each possible one-hot vector its corresponding character from decoder_dict\n",
    "one_hot_to_char = {}\n",
    "for one_hot, char in zip(inverse_one_hot_mapping, inverse_decoder_dict):\n",
    "    one_hot_to_char[one_hot] = char \n",
    "    \n",
    "# char_to_one_hot converts each character 'END', 'a', etc into a one-hot vector\n",
    "char_to_one_hot = {}\n",
    "for char, one_hot in zip(inverse_decoder_dict, inverse_one_hot_mapping): \n",
    "    char_to_one_hot[char] = one_hot\n",
    "    \n",
    "# Some examples...\n",
    "\n",
    "print(one_hot_mapping[80])\n",
    "print(inverse_one_hot_mapping[one_hot_mapping[80]])\n",
    "print(one_hot_to_char[one_hot_mapping[80]])\n",
    "print(char_to_one_hot['END'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 19\n",
    "output_size = len(decoder_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiendo funciones para el Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(img):\n",
    "    \n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    a = np.where(img != 0) \n",
    "    \n",
    "    bottom = np.max(a[0])\n",
    "    right = np.max(a[1])\n",
    "    x = np.min(a[1])\n",
    "    y = np.min(a[0])\n",
    "\n",
    "    bbox = img[y:bottom+1, x:right+1]    \n",
    "    w = bbox.shape[1]\n",
    "    h = bbox.shape[0]\n",
    "    \n",
    "    bbox_params = [x, y, w, h]\n",
    "    \n",
    "    left_margin = x\n",
    "    right_margin = width - right\n",
    "    top_margin = y\n",
    "    bottom_margin = height - bottom\n",
    "    \n",
    "    margin_sizes = [left_margin, right_margin, top_margin, bottom_margin]\n",
    "    \n",
    "    return bbox, bbox_params, margin_sizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erosion(img, kernel_size):\n",
    "    kernel = np.ones((kernel_size, kernel_size), 'uint8')\n",
    "    eroded = cv2.erode(img, kernel, iterations=1)\n",
    "    maximum = eroded.max()\n",
    "    if maximum > 0: # we need at least one non-zero pixel after eroding\n",
    "        eroded = eroded # otherwise, we won't be able to compute any bbox\n",
    "        \n",
    "    else:\n",
    "        eroded = img\n",
    "        \n",
    "    return eroded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dilation(img, margin_sizes, kernel_size):\n",
    "    pixel_excess = math.ceil((kernel_size - 1)//2) # number of pixels that might overflow the image limits\n",
    "    # after dilation\n",
    "    condition = np.any(np.array(margin_sizes) < pixel_excess) # if we overflow the image limits\n",
    "    if condition == True:\n",
    "        dilated = img # we don't do anything\n",
    "        \n",
    "    else:\n",
    "        kernel = np.ones((kernel_size, kernel_size), 'uint8')\n",
    "        dilated = cv2.dilate(img, kernel, iterations=1)\n",
    "        \n",
    "    return dilated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(img, angle):\n",
    "    rotated = ndimage.rotate(img, angle, reshape=True)\n",
    "    return rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_X(img, new_width):\n",
    "    dsize = (new_width, img.shape[0])\n",
    "    zoomed_X = cv2.resize(img, dsize, interpolation = cv2.INTER_AREA)\n",
    "    return zoomed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_Y(img, new_height):\n",
    "    dsize = (img.shape[1], new_height)\n",
    "    zoomed_Y = cv2.resize(img, dsize, interpolation = cv2.INTER_AREA)\n",
    "    return zoomed_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(new_img, src_img):\n",
    "    \n",
    "    height = src_img.shape[0]\n",
    "    new_height = new_img.shape[0]\n",
    "    width = src_img.shape[1] \n",
    "    new_width = new_img.shape[1]\n",
    "    \n",
    "    width_ratio = width/new_width\n",
    "    height_resized = math.ceil(new_height*width_ratio)\n",
    "\n",
    "    height_ratio = height/new_height\n",
    "    width_resized = math.ceil(new_width*height_ratio)\n",
    "    \n",
    "    if new_width > width and new_height > height:\n",
    "        \n",
    "        if height_resized < height:\n",
    "            \n",
    "            dsize = (width, height_resized)\n",
    "            img_rescaled = cv2.resize(new_img, dsize, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "        else:\n",
    "            dsize = (width_resized, height)\n",
    "            img_rescaled = cv2.resize(new_img, dsize, interpolation = cv2.INTER_AREA)\n",
    "            \n",
    "    elif new_width > width and new_height < height:\n",
    "        \n",
    "        dsize = (width, height_resized)\n",
    "        img_rescaled = cv2.resize(new_img, dsize, interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "    elif new_width < width and new_height > height:\n",
    "        \n",
    "        dsize = (width_resized, height)\n",
    "        img_rescaled = cv2.resize(new_img, dsize, interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        img_rescaled = new_img\n",
    "        \n",
    "        \n",
    "    return img_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting(src_img, new_bbox, bbox, bbox_params, margin_sizes):\n",
    "    \n",
    "    height = src_img.shape[0]\n",
    "    width = src_img.shape[1]\n",
    "    x, y, w, h = bbox_params # parameters of the original bounding box to put top-left edge of the new_bbox\n",
    "    # where the original was\n",
    "    left_margin, right_margin, top_margin, bottom_margin = margin_sizes\n",
    "    \n",
    "    new_height = new_bbox.shape[0]\n",
    "    new_width = new_bbox.shape[1]\n",
    "    new_img = np.zeros(src_img.shape)\n",
    "    \n",
    "    fits_vertical = new_height + top_margin < height\n",
    "    fits_horizont = new_width + left_margin < width\n",
    "\n",
    "    if fits_vertical == False and fits_horizont == False:\n",
    "        \n",
    "        diff_vertical = (new_height + top_margin) - height\n",
    "        diff_horizont = (new_width + left_margin) - width\n",
    "        new_x = x - diff_horizont\n",
    "        new_y = y - diff_vertical\n",
    "        \n",
    "        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\n",
    "        \n",
    "    elif fits_vertical == False and fits_horizont == True:\n",
    "        \n",
    "        diff_vertical = (new_height + top_margin) - height\n",
    "        new_x = x\n",
    "        new_y = y - diff_vertical\n",
    "        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\n",
    "        \n",
    "    elif fits_vertical == True and fits_horizont == False:\n",
    "        \n",
    "        diff_horizont = (new_width + left_margin) - width\n",
    "        new_x = x - diff_horizont\n",
    "        new_y = y\n",
    "        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\n",
    "        \n",
    "    else:\n",
    "        new_x = x\n",
    "        new_y = y\n",
    "        new_img[new_y:(new_y + new_height), new_x:(new_x + new_width)] = new_bbox\n",
    "        \n",
    "    new_bbox_params = [new_x, new_y, new_width, new_height]\n",
    "    \n",
    "    return new_img, new_bbox_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displacement(new_img, new_bbox_params, d_x):\n",
    "    # d_x has to be the amount of pixels\n",
    "    new_x, new_y, new_width, new_height = new_bbox_params\n",
    "    total_width = new_x + new_width + d_x\n",
    "    if total_width > new_img.shape[1]:\n",
    "        new_img_disp = new_img\n",
    "        \n",
    "    else:\n",
    "        new_img_disp = new_img\n",
    "        new_img_disp[:, d_x:] = new_img_disp[:, 0:-d_x]\n",
    "        new_img_disp[:, 0:d_x] = 0\n",
    "        \n",
    "    return new_img_disp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataAugmentation(img):\n",
    "    #img = img.numpy()\n",
    "    bbox, bbox_params, margin_sizes = get_bbox(img)\n",
    "    kernel_size = 3\n",
    "    # Sequence of possible transformations:\n",
    "    \n",
    "    # Erosion/Dilation:\n",
    "    p_er_dil = random.uniform(0,1)\n",
    "    \n",
    "    if p_er_dil < 0.33:\n",
    "        #kernel_size = random.randint(2,3)\n",
    "        new_img = erosion(img, kernel_size)\n",
    "        \n",
    "    elif p_er_dil > 0.66:\n",
    "        #kernel_size = random.randint(2,5)\n",
    "        new_img = dilation(img, margin_sizes, kernel_size)\n",
    "        \n",
    "    else:\n",
    "        new_img = img\n",
    "        \n",
    "    updated_bbox, update_bbox_params, updated_margin_sizes = get_bbox(new_img)\n",
    "    \n",
    "    #Rotation:\n",
    "    p_rot = random.uniform(0,1)\n",
    "    if (p_rot < 0.5 and updated_bbox.max() > 0): # we need to rotate a non-zero matrix...\n",
    "        angle = random.randint(-3,3)\n",
    "        new_img = rotation(updated_bbox, angle)\n",
    "        new_img, new_bbox_params, new_margin_sizes = get_bbox(new_img) # ... as we want a new bbox after rotation\n",
    "\n",
    "    else:\n",
    "        new_img = updated_bbox\n",
    "        \n",
    "    #Zoom in X axis:\n",
    "    p_zoom_X = random.uniform(0,1)\n",
    "    \n",
    "    if p_zoom_X < 0.5:\n",
    "        percentage_zoom_X = random.uniform(0.9,1.1)\n",
    "        new_width = math.ceil(percentage_zoom_X*new_img.shape[1])\n",
    "        new_img = zoom_X(new_img, new_width)\n",
    "        \n",
    "    else:\n",
    "        new_img = new_img\n",
    "        \n",
    "    #Zoom in Y axis:\n",
    "    p_zoom_Y = random.uniform(0,1)\n",
    "    \n",
    "    if p_zoom_Y < 0.5:\n",
    "        percentage_zoom_Y = random.uniform(0.9,1.1)\n",
    "        new_height = math.ceil(percentage_zoom_Y*new_img.shape[0])\n",
    "        new_img = zoom_Y(new_img, new_height)\n",
    "        \n",
    "    else:\n",
    "        new_img = new_img\n",
    "    \n",
    "    # Rescaling the new bbox in order to fit in the 48x192 original format:\n",
    "    img_rescaled = resize(new_img, img)\n",
    "    \n",
    "    # Pasting the new bbox in the 48x192 image\n",
    "    new_img, new_bbox_params = fitting(img, img_rescaled, bbox, bbox_params, margin_sizes)\n",
    "    \n",
    "    #Displacement:\n",
    "    p_disp = random.uniform(0,1)\n",
    "    \n",
    "    if p_disp < 0.7:\n",
    "        percentage_disp = random.uniform(0,0.1)\n",
    "        d_x = math.ceil(percentage_disp*img.shape[1])\n",
    "        new_img = displacement(new_img, new_bbox_params, d_x)\n",
    "        \n",
    "    else:\n",
    "        new_img = new_img\n",
    "        \n",
    "    # Salt & Pepper noise:\n",
    "    p_noise = random.uniform(0,1)\n",
    "    if p_noise < 0.5:\n",
    "        new_img = new_img/255 # normalisation\n",
    "        new_img = skimage.util.random_noise(new_img, mode = 's&p')\n",
    "\n",
    "    else:\n",
    "        new_img = new_img/255 # normalisation\n",
    "    \n",
    "    #new_img = new_img/255    \n",
    "    new_img = torch.from_numpy(new_img) # converting numpy-type image to torch tensor\n",
    "\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Data_Aug(image_set):\n",
    "    images_da = []\n",
    "    for image in image_set:\n",
    "        try:\n",
    "            image = DataAugmentation(image) # normalisation and torch.from_numpy included\n",
    "            images_da.append(image)\n",
    "            \n",
    "        except:\n",
    "            image = image / 255 # normalisation\n",
    "            image = torch.from_numpy(image) # torch.from_numpy\n",
    "            images_da.append(image)\n",
    "            \n",
    "    return images_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generando patches y etiquetas para muestra finita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_gen(image_set, batch_size, color_channels, height, width, patch_height, patch_width, stepsize):\n",
    "    total_pt = []\n",
    "    n_patches = int((width - patch_width)/stepsize + 1)\n",
    "       \n",
    "    for image in image_set:\n",
    "        \n",
    "        patches_tensor = torch.empty(n_patches, color_channels, patch_height, patch_width)    \n",
    "        start = 0\n",
    "\n",
    "        for p in range(n_patches):\n",
    "\n",
    "            patches_tensor[p, 0, :, :] = image[:, start:start + patch_width] # sliding window\n",
    "            start += stepsize # updating the bottom-left position of the patch adding the stepsize\n",
    "            \n",
    "        total_pt.append(patches_tensor)\n",
    " \n",
    "    return total_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_one_hot_target_IAM(labels, seq_len, output_size, batch_size):\n",
    "    # labels: tensor containing the labels of the words in the batch\n",
    "    # each word label consists of a vector of length 19 (MAX LENGTH). The 19 elements are the encoded characters of the word\n",
    "    # (according to Jorge's decoder dict, and completed with PADs to reach length = 19)\n",
    "    one_hot_target = torch.empty(batch_size, seq_len, output_size) # future one-hot encoding tensor for the words of the batch\n",
    "    START = inverse_decoder_dict['START'] # code number of the START token (according to Jorge's decoder_dict)\n",
    "    END = inverse_decoder_dict['END']\n",
    "    PAD = inverse_decoder_dict['PAD']\n",
    "\n",
    "    for j, word in enumerate(labels):\n",
    "        \n",
    "        It_has_PADs = torch.any(word == PAD).item() # (majority case: the label vector of the word is completed with PADs)\n",
    "        one_hot_target[j, 0, :] = one_hot_mapping[START] # START token's one-hot vector goes first\n",
    "        \n",
    "        for k, letter in enumerate(word):\n",
    "            one_hot_target[j, k + 1, :] = one_hot_mapping[letter.item()] # one-hot encoding of the rest of letters (including PADs)\n",
    "            \n",
    "        one_hot_target[j, -1, :] = one_hot_mapping[END] # last = END token\n",
    "        \n",
    "        if It_has_PADs == True: # if we had PADs\n",
    "            \n",
    "            array_of_PADs = torch.where(word == PAD)[0] \n",
    "            first_PAD = torch.min(array_of_PADs).item() # we store the first position where it appeared\n",
    "            first_PAD = first_PAD + 1 # (recall that we added the START as first element, so the indices won't match)\n",
    "            one_hot_target[j, first_PAD, :] = one_hot_mapping[END] # we replace that first PAD by an END\n",
    "            one_hot_target[j, -1, :] = one_hot_mapping[PAD] # then the last element was a PAD, and not the END token\n",
    "            \n",
    "    return one_hot_target\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_conversion(decoder_output, output_size):\n",
    "    \n",
    "    one_hot_output_letter = torch.zeros(1, 1, output_size)\n",
    "    index = torch.argmax(decoder_output, dim = 2).item()\n",
    "    one_hot_output_letter[0, 0, index] = 1.\n",
    "    \n",
    "    return one_hot_output_letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generando datos por batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_batch(set_random_sample, set_length, batch_size):\n",
    "    sorted_set_rs = []\n",
    "    j = 0\n",
    "    while (j + batch_size < set_length):\n",
    "        sorted_set_rs.append(np.sort(set_random_sample[j:j+batch_size]))\n",
    "        j = j + batch_size\n",
    "    \n",
    "    sorted_set_rs.append(np.sort(set_random_sample[j:]))\n",
    "    sorted_set_rs = np.concatenate(sorted_set_rs)\n",
    "    return sorted_set_rs\n",
    "\n",
    "def data_generator(batch_size, len_set_IAM, len_set_OSB, image_set, target_set, random_sampling_IAM, random_sampling_OSB, mode):\n",
    "    \n",
    "    f_IAM = h5py.File(path_IAM, \"r\")\n",
    "    f_OSB = h5py.File(path_OSB, \"r\")\n",
    "    \n",
    "    IAM_times = 0 # times we have covered the IAM dataset during this epoch\n",
    "    OSB_times = 0 # times we have covered the OSB dataset during this epoch\n",
    "    j = 0\n",
    "    k = 0\n",
    "\n",
    "    while 1:\n",
    "        IAM = False\n",
    "        OSB = False\n",
    "        if random.random() > 0.125:\n",
    "            IAM = True\n",
    "            indices = random_sampling_IAM[j:j+batch_size]\n",
    "            data_X = f_IAM[image_set][indices]\n",
    "            data_y = f_IAM[target_set][indices]\n",
    "                       \n",
    "        else:\n",
    "            OSB = True\n",
    "            indices = random_sampling_OSB[k:k+batch_size]\n",
    "            data_X = f_OSB[image_set][indices]\n",
    "            data_y = f_OSB[target_set][indices]\n",
    "        \n",
    "        if mode == 'training':\n",
    "            data_X = get_Data_Aug(data_X)\n",
    "            \n",
    "        elif mode == 'training_without_data_aug':\n",
    "            data_X = data_X/255\n",
    "            data_X = torch.from_numpy(data_X)\n",
    "        \n",
    "        elif mode == 'validation':\n",
    "            data_X = data_X/255\n",
    "            data_X = torch.from_numpy(data_X)\n",
    "        \n",
    "        # Getting patches from images (data augmented or not)\n",
    "        data_X = patch_gen(image_set = data_X, batch_size = batch_size, color_channels = 1, \n",
    "                              height = 48, width = 192, patch_height = 48, patch_width = 10, stepsize = 2)\n",
    "        data_X = torch.cat(data_X, dim = 0) # stacking patches tensors of the whole batch along the dim 0 (CNN input)\n",
    "        \n",
    "        data_y = torch.from_numpy(data_y) # converting numpy objects to PyTorch tensors\n",
    "        data_y[data_y == 100.] = 80. # replacing Jorge's coding of PAD token (100.) by ours (80.)\n",
    "        \n",
    "        yield data_X, data_y\n",
    "        \n",
    "        # covered_data: ~ number of patterns that the network has seen during this epoch;  OSB (~7.000 training patterns) \n",
    "        # might have been picked and covered several times during 1 epoch (57.000 training patterns):\n",
    "        covered_data = (IAM_times*len_set_IAM) + (OSB_times*len_set_OSB) + (j + k + 2*batch_size) \n",
    "        data_length = len_set_IAM + len_set_OSB # total number of patterns for both datasets combined\n",
    "        \n",
    "        if covered_data >= data_length: # epoch has been completed\n",
    "            j = 0\n",
    "            k = 0\n",
    "            IAM_times = 0\n",
    "            OSB_times = 0\n",
    "            break\n",
    "        \n",
    "        else: # if not, parameters for each dataset are updated\n",
    "            if IAM == True:\n",
    "                if j + 2*batch_size >= len_set_IAM: # drop_last = True\n",
    "                    j = 0\n",
    "                    IAM_times += 1\n",
    "\n",
    "                else:\n",
    "                    j += batch_size\n",
    "\n",
    "            if OSB == True:     \n",
    "                if k + 2*batch_size >= len_set_OSB: # drop_last = True\n",
    "                    k = 0\n",
    "                    OSB_times += 1\n",
    "                else:\n",
    "                    k += batch_size\n",
    "        \n",
    "    f_IAM.close()\n",
    "    f_OSB.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiendo la arquitectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, IN_CHANNELS, FILTERS_CNN_1, FILTERS_CNN_2, NEURONS_IN_DENSE_LAYER,\n",
    "                 PATCH_HEIGHT, PATCH_WIDTH, STRIDE, PADDING, KERNEL_SIZE, dropout_p):\n",
    "        super().__init__()\n",
    "        self.IN_CHANNELS = IN_CHANNELS\n",
    "        self.FILTERS_CNN_1 = FILTERS_CNN_1\n",
    "        self.FILTERS_CNN_2 = FILTERS_CNN_2\n",
    "        self.NEURONS_IN_DENSE_LAYER = NEURONS_IN_DENSE_LAYER\n",
    "        self.PATCH_HEIGHT_AFTER_POOLING = PATCH_HEIGHT//4\n",
    "        self.PATCH_WIDTH_AFTER_POOLING = PATCH_WIDTH//4\n",
    "        self.STRIDE = STRIDE\n",
    "        self.PADDING = PADDING\n",
    "        self.KERNEL_SIZE = KERNEL_SIZE\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = self.IN_CHANNELS, out_channels = self.FILTERS_CNN_1,\n",
    "                               kernel_size = self.KERNEL_SIZE, stride = self.STRIDE, padding = self.PADDING)\n",
    "        self.conv2 = nn.Conv2d(in_channels = self.FILTERS_CNN_1, out_channels = self.FILTERS_CNN_2,\n",
    "                               kernel_size = self.KERNEL_SIZE, stride = self.STRIDE, padding = self.PADDING)\n",
    "        self.fc1 = nn.Linear(self.PATCH_HEIGHT_AFTER_POOLING * self.PATCH_WIDTH_AFTER_POOLING * self.FILTERS_CNN_2, \n",
    "                             self.NEURONS_IN_DENSE_LAYER)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu((self.conv1(X)))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = F.relu((self.conv2(X)))\n",
    "        X = F.max_pool2d(X, 2, 2)\n",
    "        X = X.view(-1, self.PATCH_HEIGHT_AFTER_POOLING*self.PATCH_WIDTH_AFTER_POOLING*self.FILTERS_CNN_2)\n",
    "        X = self.dropout(self.fc1(X))\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, encoder_seq_len, num_layers, num_directions, dropout_p):        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = encoder_seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.num_directions = num_directions\n",
    "        self.dropout = dropout_p\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first = True, dropout = self.dropout, \n",
    "                           bidirectional = True)\n",
    "\n",
    "    def forward(self, input, hidden):        \n",
    "        output = input.view(self.batch_size, self.seq_len, self.input_size)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(self.num_layers * self.num_directions, self.batch_size, self.hidden_size, device=device),\n",
    "                torch.zeros(self.num_layers * self.num_directions, self.batch_size, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size, hidden_size, dropout_p, batch_size, encoder_seq_len, decoder_seq_len):\n",
    "        super(BahdanauDecoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.encoder_seq_len = encoder_seq_len\n",
    "        self.decoder_seq_len = decoder_seq_len\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        #self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.fc_encoder = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "        self.weight_vector = torch.FloatTensor(self.batch_size, self.hidden_size, self.decoder_seq_len)\n",
    "        self.weight = nn.Parameter(nn.init.xavier_uniform_(self.weight_vector)) #xavier initializer avoids nans\n",
    "        #self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.output_size + self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, inputs, hidden, encoder_outputs):\n",
    "        #encoder_outputs = encoder_outputs.squeeze()\n",
    "        # Embed input words\n",
    "        #embedded = self.embedding(inputs).view(1, -1)\n",
    "        #embedded = self.dropout(embedded)\n",
    "\n",
    "        # Calculating Alignment Scores\n",
    "        hidden_state = hidden[0].view(self.batch_size, self.decoder_seq_len, self.hidden_size)\n",
    "        x = torch.tanh(self.fc_hidden(hidden_state) + self.fc_encoder(encoder_outputs))\n",
    "\n",
    "        alignment_scores = torch.bmm(x, self.weight)\n",
    "        alignment_scores = alignment_scores.view(self.batch_size, self.decoder_seq_len, self.encoder_seq_len)\n",
    " \n",
    "        # Softmaxing alignment scores to get Attention weights\n",
    "        attn_weights = F.softmax(alignment_scores, dim = 2)\n",
    "\n",
    "        # Multiplying the Attention weights with encoder outputs to get the context vector\n",
    "        context_vector = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "        # Concatenating context vector with embedded input word\n",
    "        output = torch.cat((inputs, context_vector), 2)\n",
    "        # Passing the concatenated vector as input to the LSTM cell\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        # Passing the LSTM output through a Linear layer acting as a classifier\n",
    "        output = F.log_softmax(self.classifier(output), dim = 2)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "\n",
    "CNN_model = ConvolutionalNetwork(IN_CHANNELS = 1, FILTERS_CNN_1 = 20, FILTERS_CNN_2 = 50, NEURONS_IN_DENSE_LAYER = 1024, \n",
    "                                 PATCH_HEIGHT = 48, PATCH_WIDTH = 10, STRIDE = 1, PADDING = 2, KERNEL_SIZE = 5, dropout_p = 0.5).cuda(1)\n",
    "CNN_optimizer = torch.optim.Adam(CNN_model.parameters(), lr = 0.001)\n",
    "CNN_scheduler = torch.optim.lr_scheduler.StepLR(CNN_optimizer, step_size = 1, gamma = 0.98) # decreasing lr 2% every epoch\n",
    "\n",
    "Encoder_model = EncoderRNN(input_size = 1024, hidden_size = 256, batch_size = 256, encoder_seq_len = 92, \n",
    "                           num_layers = 2, num_directions = 2, dropout_p = 0.5).cuda(1)\n",
    "Encoder_optimizer = torch.optim.Adam(Encoder_model.parameters(), lr = 0.001)\n",
    "Encoder_scheduler = torch.optim.lr_scheduler.StepLR(Encoder_optimizer, step_size = 1, gamma = 0.98)\n",
    "\n",
    "#Decoder_model = AttnDecoderRNN(output_size = len(decoder_dict), hidden_size = 256, dropout_p = 0, batch_size = 256,\n",
    "#                               encoder_seq_len = 92, decoder_seq_len = 1).cuda(1)\n",
    "Decoder_model = BahdanauDecoder(output_size = len(decoder_dict), hidden_size = 256, dropout_p = 0, batch_size = 256,\n",
    "                               encoder_seq_len = 92, decoder_seq_len = 1).cuda(1)\n",
    "Decoder_optimizer = torch.optim.Adam(Decoder_model.parameters(), lr = 0.001)\n",
    "Decoder_scheduler = torch.optim.lr_scheduler.StepLR(Decoder_optimizer, step_size = 1, gamma = 0.98)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_losses = []\n",
    "    \n",
    "    random_sampling_IAM = random.sample(range(len_trn_IAM), len_trn_IAM)\n",
    "    random_sampling_IAM = sort_by_batch(random_sampling_IAM, len_trn_IAM, batch_size)\n",
    "    \n",
    "    random_sampling_OSB = random.sample(range(len_trn_OSB), len_trn_OSB)\n",
    "    random_sampling_OSB = sort_by_batch(random_sampling_OSB, len_trn_OSB, batch_size)\n",
    "\n",
    "    train_loader = data_generator(batch_size, len_trn_IAM, len_trn_OSB, image_set = 'X_trn', target_set = 'target_trn',\n",
    "                                  random_sampling_IAM = random_sampling_IAM, random_sampling_OSB = random_sampling_OSB, \n",
    "                                  mode = 'training')\n",
    "    \n",
    "    for num_batch, (images, labels) in enumerate(train_loader):         \n",
    "        num_batch += 1\n",
    "\n",
    "        encoder_hidden = Encoder_model.initHidden()\n",
    "        images = images.cuda(1)\n",
    "        encoder_input = CNN_model(images)\n",
    "        encoder_outputs, encoder_hidden = Encoder_model(encoder_input, encoder_hidden)\n",
    "        encoder_outputs = encoder_outputs.reshape(batch_size, encoder_seq_len, enc_num_directions*enc_hidden_size)\n",
    "        encoder_outputs = encoder_outputs.view(batch_size, encoder_seq_len, enc_num_directions, enc_hidden_size)\n",
    "        encoder_outputs = encoder_outputs[:, :, 0, :] + encoder_outputs[:, :, 1, :]\n",
    "        encoder_outputs = encoder_outputs.view(batch_size, encoder_seq_len, enc_hidden_size)\n",
    "\n",
    "        #hidden_states = encoder_outputs[:, -1, 0, :].view(1, 256, 256)\n",
    "        #hidden_last = hidden_states[:, 0, :].view(1,1,256)\n",
    "        #h_n = encoder_hidden[0][2, 0, :].view(1,1,256)\n",
    "        #print(hidden_last)\n",
    "        #print(h_n)\n",
    "        #decoder_hidden = encoder_hidden\n",
    "\n",
    "        #decoder_hidden = (encoder_hidden[0][-2, :, :].unsqueeze(0), encoder_hidden[1][-2, :, :].unsqueeze(0)) # last encoder layer (and forward) hidden state\n",
    "        hidden_state = encoder_hidden[0][-2, :, :].view(1, batch_size, enc_hidden_size) + encoder_hidden[0][-1, :, :].view(1, batch_size, enc_hidden_size)\n",
    "        cell_state = encoder_hidden[1][-2, :, :].view(1, batch_size, enc_hidden_size) + encoder_hidden[1][-1, :, :].view(1, batch_size, enc_hidden_size)\n",
    "        decoder_hidden = (hidden_state, cell_state)\n",
    "        decoder_input = get_one_hot_target_IAM(labels=labels, seq_len = MAX_LENGTH + 2, output_size = output_size, batch_size = 256).cuda(1)\n",
    "        \n",
    "        decoder_output_total = []\n",
    "        for num_letter in range(MAX_LENGTH + 2):\n",
    "            \n",
    "            decoder_input_letter = decoder_input[:, num_letter, :].unsqueeze(1)\n",
    "            \n",
    "            decoder_output, decoder_hidden, attn_weights = Decoder_model(decoder_input_letter, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_output_total.append(decoder_output)\n",
    "            \n",
    "        decoder_output_total = torch.cat(decoder_output_total, dim = 1)\n",
    "        \n",
    "        \n",
    "        output_indices = torch.tensor(list(range(0, MAX_LENGTH + 2 -1))).cuda(1) # removing last token from the output\n",
    "        decoder_output = torch.index_select(decoder_output_total, dim = 1, index = output_indices)\n",
    "\n",
    "        ground_truth = torch.argmax(decoder_input, dim = 2)\n",
    "        target_indices = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(1) # remove SOS token from the input\n",
    "        ground_truth = torch.index_select(ground_truth, dim = 1, index = target_indices)\n",
    "        \n",
    "        decoder_output = decoder_output.view(batch_size*(MAX_LENGTH + 1), output_size)\n",
    "        ground_truth = ground_truth.view(batch_size*(MAX_LENGTH + 1))\n",
    "\n",
    "        loss = criterion(decoder_output, ground_truth)\n",
    "        \n",
    "        CNN_optimizer.zero_grad()\n",
    "        Encoder_optimizer.zero_grad()\n",
    "        Decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        CNN_optimizer.step()\n",
    "        Encoder_optimizer.step()\n",
    "        Decoder_optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "    \n",
    "    valid_losses = []\n",
    "    \n",
    "    random_sampling_val_IAM = random.sample(range(len_val_IAM), len_val_IAM)\n",
    "    random_sampling_val_IAM = sort_by_batch(random_sampling_val_IAM, len_val_IAM, batch_size)\n",
    "    \n",
    "    random_sampling_val_OSB = random.sample(range(len_val_OSB), len_val_OSB)\n",
    "    random_sampling_val_OSB = sort_by_batch(random_sampling_val_OSB, len_val_OSB, batch_size)\n",
    "    \n",
    "    val_loader = data_generator(batch_size, len_val_IAM, len_val_OSB, image_set = 'X_val', target_set = 'target_val',\n",
    "                                random_sampling_IAM = random_sampling_val_IAM, random_sampling_OSB = random_sampling_val_OSB, \n",
    "                                mode = 'validation')\n",
    "    \n",
    "    with torch.no_grad():       \n",
    "        for num_batch_val, (images_val, labels_val) in enumerate(val_loader):        \n",
    "            num_batch_val += 1\n",
    "            encoder_hidden_val = Encoder_model.initHidden()\n",
    "            images_val = images_val.cuda(1)\n",
    "            encoder_input_val = CNN_model(images_val)\n",
    "            encoder_outputs_val, encoder_hidden_val = Encoder_model(encoder_input_val, encoder_hidden_val)\n",
    "            encoder_outputs_val = encoder_outputs_val.reshape(batch_size, encoder_seq_len, enc_num_directions*enc_hidden_size)\n",
    "            encoder_outputs_val = encoder_outputs_val.view(batch_size, encoder_seq_len, enc_num_directions, enc_hidden_size)\n",
    "            encoder_outputs_val = encoder_outputs_val[:, :, 0, :] + encoder_outputs_val[:, :, 1, :]\n",
    "            \n",
    "            #decoder_hidden_val = (encoder_hidden_val[0][-2, :, :].unsqueeze(0), encoder_hidden_val[1][-2, :, :].unsqueeze(0)) # last encoder layer (and forward) hidden state\n",
    "            hidden_state_val = encoder_hidden_val[0][-2, :, :].view(1, batch_size, enc_hidden_size) + encoder_hidden_val[0][-1, :, :].view(1, batch_size, enc_hidden_size)\n",
    "            cell_state_val = encoder_hidden_val[1][-2, :, :].view(1, batch_size, enc_hidden_size) + encoder_hidden_val[1][-1, :, :].view(1, batch_size, enc_hidden_size)\n",
    "            decoder_hidden_val = (hidden_state_val, cell_state_val)\n",
    "            decoder_input_val = get_one_hot_target_IAM(labels=labels_val, seq_len = MAX_LENGTH + 2, output_size = output_size, batch_size = 256).cuda(1)\n",
    "            \n",
    "            decoder_output_total_val = []\n",
    "            for num_letter_val in range(MAX_LENGTH + 2):\n",
    "            \n",
    "                decoder_input_letter_val = decoder_input_val[:, num_letter_val, :].unsqueeze(1)\n",
    "\n",
    "                decoder_output_val, decoder_hidden_val, attn_weights_val = Decoder_model(decoder_input_letter_val, decoder_hidden_val, encoder_outputs_val)\n",
    "                \n",
    "                decoder_output_total_val.append(decoder_output_val)\n",
    "                \n",
    "            decoder_output_total_val = torch.cat(decoder_output_total_val, dim = 1)\n",
    "            \n",
    "            output_indices_val = torch.tensor(list(range(0, MAX_LENGTH + 2 - 1))).cuda(1) # remove last token from the output\n",
    "            decoder_output_val = torch.index_select(decoder_output_total_val, dim = 1, index = output_indices_val)\n",
    "\n",
    "            ground_truth_val = torch.argmax(decoder_input_val, dim = 2)\n",
    "            target_indices_val = torch.tensor(list(range(1, MAX_LENGTH + 2))).cuda(1) # remove START token from the input\n",
    "            ground_truth_val = torch.index_select(ground_truth_val, dim = 1, index = target_indices_val)\n",
    "            \n",
    "            \n",
    "            decoder_output_val = decoder_output_val.view(batch_size*(MAX_LENGTH + 1), output_size)\n",
    "            ground_truth_val = ground_truth_val.view(batch_size*(MAX_LENGTH + 1))\n",
    "\n",
    "            loss_val = criterion(decoder_output_val, ground_truth_val)\n",
    "            valid_losses.append(loss_val.item())\n",
    "    return np.mean(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patience():\n",
    "    \n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.current_patience = patience\n",
    "        self.min_loss_val = float('inf')\n",
    "\n",
    "    def more_patience(self,loss_val):\n",
    "        self.current_patience -= 1\n",
    "        if self.current_patience == 0:\n",
    "            return False\n",
    "\n",
    "        if loss_val < self.min_loss_val:\n",
    "            self.min_loss_val = loss_val\n",
    "            self.current_patience = patience\n",
    "\n",
    "            model_name = f\"2BILSTM_Bahdanau_IAM_OSBORNE\"\n",
    "            print(\", saved best model.\")\n",
    "            \n",
    "            torch.save({\n",
    "                'CNN_model_state_dict_IAM_OSB': CNN_model.state_dict(),\n",
    "                'CNN_optimizer_state_dict_IAM_OSB': CNN_optimizer.state_dict(),\n",
    "                'Encoder_model_state_dict_IAM_OSB': Encoder_model.state_dict(),\n",
    "                'Encoder_optimizer_state_dict_IAM_OSB': Encoder_optimizer.state_dict(),\n",
    "                'Decoder_model_state_dict_IAM_OSB': Decoder_model.state_dict(),\n",
    "                'Decoder_optimizer_state_dict_IAM_OSB': Decoder_optimizer.state_dict(),\n",
    "            }, 'Attention_IAM_OSB'+model_name)\n",
    "            \n",
    "            torch.save(CNN_model.state_dict(), 'CNN_'+model_name)\n",
    "            torch.save(Encoder_model.state_dict(), 'Encoder_'+model_name)\n",
    "            torch.save(Decoder_model.state_dict(), 'Decoder_'+model_name)\n",
    "    \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train loss: 0.9544492380641331 Valid loss: 0.6936579902966817 Duration: 3.2211692690849305 minutes\n",
      ", saved best model.\n",
      "Epoch: 1 Train loss: 0.6971235216778016 Valid loss: 0.6151993115743001 Duration: 6.468083480993907 minutes\n",
      ", saved best model.\n",
      "Epoch: 2 Train loss: 0.6182667629741062 Valid loss: 0.5430951019128164 Duration: 9.71184621254603 minutes\n",
      ", saved best model.\n",
      "Epoch: 3 Train loss: 0.5773695260286331 Valid loss: 0.5342610627412796 Duration: 12.957438496748606 minutes\n",
      ", saved best model.\n",
      "Epoch: 4 Train loss: 0.553491602294913 Valid loss: 0.48882017433643343 Duration: 16.207496503988903 minutes\n",
      ", saved best model.\n",
      "Epoch: 5 Train loss: 0.5229471952558677 Valid loss: 0.45822919805844625 Duration: 19.45602447191874 minutes\n",
      ", saved best model.\n",
      "Epoch: 6 Train loss: 0.4994939879000744 Valid loss: 0.4619922677675883 Duration: 22.712537145614625 minutes\n",
      "Epoch: 7 Train loss: 0.47409003569143954 Valid loss: 0.4270054340362549 Duration: 25.961822581291198 minutes\n",
      ", saved best model.\n",
      "Epoch: 8 Train loss: 0.4607187228225102 Valid loss: 0.41454351842403414 Duration: 29.22747797568639 minutes\n",
      ", saved best model.\n",
      "Epoch: 9 Train loss: 0.43066886666222154 Valid loss: 0.40392882865050744 Duration: 32.57544074058533 minutes\n",
      ", saved best model.\n",
      "Epoch: 10 Train loss: 0.41203874029288784 Valid loss: 0.36672633786996206 Duration: 35.93609651327133 minutes\n",
      ", saved best model.\n",
      "Epoch: 11 Train loss: 0.39805292136201237 Valid loss: 0.3588205595811208 Duration: 39.29142620166143 minutes\n",
      ", saved best model.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "patience = 150\n",
    "\n",
    "patience_controler = Patience(patience)\n",
    "start_time = time.time()\n",
    "\n",
    "len_trn_IAM = 47926\n",
    "len_val_IAM = 7558\n",
    "len_tst_IAM = 20292\n",
    "\n",
    "len_trn_OSB = 7149\n",
    "len_val_OSB = 286\n",
    "len_tst_OSB = 194\n",
    "\n",
    "encoder_seq_len = 92\n",
    "enc_num_directions = 2\n",
    "batch_size = 256\n",
    "enc_hidden_size = 256\n",
    "dec_hidden_size = 256\n",
    "#hidden_size = 256\n",
    "\n",
    "for num_epoch in range(5000000):\n",
    "\n",
    "    train_loss = train()        \n",
    "    valid_loss = validation()\n",
    "    \n",
    "    CNN_scheduler.step()\n",
    "    Encoder_scheduler.step()\n",
    "    Decoder_scheduler.step()\n",
    "    \n",
    "    writer.add_scalar('Loss/train', train_loss, num_epoch)\n",
    "    writer.add_scalar('Loss/validation', valid_loss, num_epoch)\n",
    "    \n",
    "    print(f'Epoch: {num_epoch} Train loss: {train_loss} Valid loss: {valid_loss} Duration: {(time.time() - start_time)/60} minutes',)\n",
    "\n",
    "    if not patience_controler.more_patience(valid_loss):\n",
    "        print(\"Se acabó la paciencia\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activar pytorch_estoril (environment) en la terminal y ejecutar tensorboard --host 0.0.0.0 --logdir ./runs\n",
    "# Tensorboard se ejecutará en un cierto puerto y nos dará el enlace. Habrá que sustituir la IP 0.0.0.0 por la del equipo\n",
    "# en remoto en la que esté corriendo en el caso de Estoril 212.128.3.86:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_estoril",
   "language": "python",
   "name": "pytorch_estoril"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
